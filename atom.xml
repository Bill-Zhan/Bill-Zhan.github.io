<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Bill-Zhan.github.io</id>
    <title>Gridea</title>
    <updated>2021-01-19T02:03:35.224Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://Bill-Zhan.github.io"/>
    <link rel="self" href="https://Bill-Zhan.github.io/atom.xml"/>
    <subtitle>Live to Learn</subtitle>
    <logo>https://Bill-Zhan.github.io/images/avatar.png</logo>
    <icon>https://Bill-Zhan.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[XGBoost]]></title>
        <id>https://Bill-Zhan.github.io/post/xgboost/</id>
        <link href="https://Bill-Zhan.github.io/post/xgboost/">
        </link>
        <updated>2021-01-18T13:02:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="xgboost-extreme-gradient-boost">XGBoost: Extreme Gradient Boost</h1>
<h2 id="1-xgboost-contents">1. XGBoost Contents</h2>
<ol>
<li>Gradient Boost</li>
<li>Regularization</li>
<li>Unique Regression Tree/ Classification Tree</li>
<li>Approximate Greedy Algorithm</li>
<li>Weighted Quantile Sketch</li>
<li>Sparsity-Aware Split Finding</li>
<li>Paralle Learning</li>
<li>Cache-Aware Access</li>
<li>Blocks for Out-of-Core Computation</li>
</ol>
<h2 id="2-xgboost-details">2. XGBoost Details</h2>
<h4 id="21-xgboost-for-regression">2.1 XGBoost for Regression</h4>
<h6 id="211-example">2.1.1 Example</h6>
<figure data-type="image" tabindex="1"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_example.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>Make initial prediction</p>
<ul>
<li>By defalut use <code>$0.5$</code>.</li>
<li>Can use average (mean).</li>
</ul>
</li>
<li>
<p>Calculate similarity score:</p>
<pre><code class="language-math">\text{Similarity Score} = \frac{(\sum residuals)^2}{\#residuals + \lambda}
</code></pre>
<ul>
<li><code>$\lambda$</code>: <strong>regularization parameter</strong><br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_lambda.png" alt="" loading="lazy"></li>
<li>the residuals can <strong>cancel out</strong> in the score.</li>
</ul>
</li>
<li>
<p>Try all valid splits and calculate similarity scores for each child.</p>
<ul>
<li>nodes with quite different residuals -&gt; many cancel out -&gt; low similarity scores</li>
</ul>
</li>
<li>
<p>Calculate <code>$Gain = left + right - root$</code>, on similarity scores.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_similarity_gain.png" alt="" loading="lazy"></p>
</li>
<li>
<p>Compare and select the split that has the largest similarity gain.</p>
</li>
<li>
<p>Prune XGBoost tree with <code>$\gamma$</code>, a <strong>tree complexity parameter</strong>.</p>
</li>
</ol>
<pre><code class="language-math">Gain-\gamma \;
\begin{cases}
      &lt;0 &amp; \text{reomve the node}\\
      &gt;0 &amp; \text{keep the node}
\end{cases} 
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_gain_lambda.png" alt="" loading="lazy"></figure>
<ol start="7">
<li>Calculate the output value of each leaf<pre><code class="language-math">\text{Output Value} = \frac{\sum residuals}{\#residuals + \lambda}
</code></pre>
<ul>
<li>It's exaclty the same as the output value in <strong>GBDT</strong> except for the <strong>regularization parameter</strong>.</li>
<li>if <code>$\lambda&gt;0$</code> -&gt; reduce the amount of the leaf output adds to the prediction -&gt; make the prediction <strong>less sensitive</strong>.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_output.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>Keep adding trees to predict latest residuals, until reaching the maximum steps, or adding new trees doesn't significantly reduce residuals.</li>
</ol>
<h6 id="212-algorithm">2.1.2 Algorithm</h6>
<h4 id="22-xgboost-for-classification">2.2 XGBoost for Classification</h4>
<h6 id="221-example">2.2.1 Example</h6>
<figure data-type="image" tabindex="3"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_clf_example.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>Make initial prediction</p>
<ul>
<li>By defalut use <code>$0.5$</code>.</li>
<li>Can use average (mean) probability.</li>
</ul>
</li>
<li>
<p>Calculate similarity score:</p>
<pre><code class="language-math">\text{Similarity Score} = \frac{(\sum residuals)^2}{\sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)] + \lambda}
</code></pre>
<ul>
<li>The numerator is the same as that in regression.</li>
<li><code>$\lambda$</code>: <strong>regularization parameter</strong><br>
<img src="" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>Try all valid splits and calculate similarity scores for each child.</p>
</li>
<li>
<p>Calculate <code>$Gain = left + right - root$</code>, on similarity scores.</p>
</li>
<li>
<p>Compare and select the split that has the largest similarity gain.</p>
</li>
<li>
<p>Prune XGBoost tree with</p>
<ol>
<li><strong>cover</strong> = (<strong>min_child_weight</strong>?): determine minimum number of residuals in a leaf.<pre><code class="language-math">cover = \sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)]
</code></pre>
<ul>
<li>This is the <strong>Gini-Impurity</strong>.</li>
<li>For regression: <code>$cover = \# residuals$</code></li>
<li>Generally, <code>$cover = denomiator\;of\;similarity-\lambda$</code>, by default is 1.</li>
</ul>
</li>
<li>Also use <code>$gamma$</code>.</li>
</ol>
</li>
<li>
<p>Calculate the output value of each leaf</p>
<pre><code class="language-math">\text{Output Value} = \frac{(\sum residuals)}{\sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)] + \lambda}
</code></pre>
<ul>
<li>It's exaclty the same as the output value in <strong>GBDT</strong> except for the <strong>regularization parameter</strong>.</li>
<li>if <code>$\lambda&gt;0$</code> -&gt; reduce the amount of the leaf output adds to the prediction -&gt; make the prediction <strong>less sensitive</strong>.<br>
<img src="" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>Keep adding trees to predict latest residuals, until reaching the maximum steps, or adding new trees doesn't significantly reduce residuals.</p>
</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_clf_output.png" alt="" loading="lazy"></figure>
<h6 id="222-algorithm">2.2.2 Algorithm</h6>
<h2 id="3-xgboost-implementationoptimization">3. XGBoost Implementation(optimization)</h2>
<h4 id="31-approximate-greedy-algorithm">3.1 Approximate Greedy Algorithm</h4>
<ul>
<li>
<p>When searching for <strong>best splitting point</strong>, XGBoost uses <strong>greedy</strong> strategy, which doesn't take long-term outcomes. This makes the training fast.</p>
</li>
<li>
<p>It first finds the <strong>quantiles</strong> of a feature, and use them to define splitting thresholds.</p>
</li>
</ul>
<h4 id="32-parallel-learning">3.2 Parallel Learning</h4>
<p>Quantile Sketch Algorithm:</p>
<ol>
<li>Data in different machine -&gt; Approximate histogram -&gt; Approximate quantiles</li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/quantile_sketched_algorithm.png" alt="" loading="lazy"></figure>
<ol start="2">
<li>Weighted quantile: the sum of __ weights__ is the same in each quantile, which is defined as the <strong>cover</strong> &lt;-&gt; second derivative.
<ul>
<li>Regression: is all <strong>1</strong>, just like the normal quantile</li>
<li>Classification: <code>$Previous\;Probability_i(1-Previous\;Probability_i)$</code></li>
</ul>
</li>
</ol>
<h4 id="33-sparsity-aware-split-finding">3.3 Sparsity-Aware Split Finding</h4>
<ol>
<li>
<p>Find splitting point<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_missing_split.png" alt="" loading="lazy"></p>
</li>
<li>
<p>Use the strategy for all missing data in the future.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_missing_split2.png" alt="" loading="lazy"></p>
</li>
</ol>
<h4 id="34-cache-aware-access">3.4 Cache-Aware Access</h4>
<figure data-type="image" tabindex="6"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_cache_memory.png" alt="" loading="lazy"></figure>
<h2 id="4-xgboost-application">4. XGBoost Application</h2>
<h4 id="41-advantages-vs-gbm">4.1 Advantages vs. GBM</h4>
<ol>
<li>
<p>Regularization:</p>
<ul>
<li>Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.</li>
<li>In fact, XGBoost is also known as a â€˜regularized boostingâ€˜ technique.</li>
</ul>
</li>
<li>
<p>Parallel Processing:</p>
<ul>
<li>XGBoost implements parallel processing and is blazingly faster as compared to GBM.</li>
<li>But hang on, we know that boosting is a sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where Iâ€™m coming from. Check this link out to explore further.</li>
<li>XGBoost also supports implementation on Hadoop.</li>
</ul>
</li>
<li>
<p>High Flexibility</p>
<ul>
<li>XGBoost allows users to define custom optimization objectives and evaluation criteria.</li>
<li>This adds a whole new dimension to the model and there is no limit to what we can do.</li>
</ul>
</li>
<li>
<p>Handling Missing Values</p>
<ul>
<li>XGBoost has an in-built routine to handle missing values.</li>
<li>The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.</li>
</ul>
</li>
<li>
<p>Tree Pruning:</p>
<ul>
<li>A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.</li>
<li>XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.</li>
<li>Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.</li>
</ul>
</li>
<li>
<p>Built-in Cross-Validation</p>
<ul>
<li>XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.</li>
<li>This is unlike GBM where we have to run a grid-search and only a limited values can be tested.</li>
</ul>
</li>
<li>
<p>Continue on Existing Model</p>
<ul>
<li>User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.</li>
<li>GBM implementation of sklearn also has this feature so they are even on this point.</li>
</ul>
</li>
</ol>
<h4 id="42-xgboost-parameters">4.2 XGBoost Parameters</h4>
<p>https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://Bill-Zhan.github.io/post/hello-gridea/</id>
        <link href="https://Bill-Zhan.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea ä¸»é¡µ</a><br>
<a href="http://fehey.com/">ç¤ºä¾‹ç½‘ç«™</a></p>
<h2 id="ç‰¹æ€§">ç‰¹æ€§ğŸ‘‡</h2>
<p>ğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ <strong>Markdown</strong> è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ</p>
<p>ğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡</p>
<p>ğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„</p>
<p>ğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•</p>
<p>ğŸ’»  ä½ å¯ä»¥åœ¨ <strong>Windows</strong>ï¼Œ<strong>MacOS</strong> æˆ– <strong>Linux</strong> è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯</p>
<p>ğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ <strong>ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ</strong> æˆ– <strong>Coding Pages</strong> å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°</p>
<p>ğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ <a href="https://github.com/gitalk/gitalk">Gitalk</a> æˆ– <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> è¯„è®ºç³»ç»Ÿ</p>
<p>ğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨<strong>ä¸­æ–‡ç®€ä½“</strong>æˆ–<strong>è‹±è¯­</strong></p>
<p>ğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜ï¼Œå¼ºå¤§çš„ä¸»é¢˜è‡ªå®šä¹‰èƒ½åŠ›</p>
<p>ğŸ–¥  ä½ å¯ä»¥è‡ªå®šä¹‰æºæ–‡ä»¶å¤¹ï¼Œåˆ©ç”¨ OneDriveã€ç™¾åº¦ç½‘ç›˜ã€iCloudã€Dropbox ç­‰è¿›è¡Œå¤šè®¾å¤‡åŒæ­¥</p>
<p>ğŸŒ± å½“ç„¶ <strong>Gridea</strong> è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ ğŸƒ</p>
<p>æœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´</p>
<p>å°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼</p>
<p>ğŸ˜˜ Enjoy~</p>
]]></content>
    </entry>
</feed>