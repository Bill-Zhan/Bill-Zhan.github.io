<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Bill-Zhan.github.io</id>
    <title>Gridea</title>
    <updated>2021-01-19T02:03:35.224Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://Bill-Zhan.github.io"/>
    <link rel="self" href="https://Bill-Zhan.github.io/atom.xml"/>
    <subtitle>Live to Learn</subtitle>
    <logo>https://Bill-Zhan.github.io/images/avatar.png</logo>
    <icon>https://Bill-Zhan.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[XGBoost]]></title>
        <id>https://Bill-Zhan.github.io/post/xgboost/</id>
        <link href="https://Bill-Zhan.github.io/post/xgboost/">
        </link>
        <updated>2021-01-18T13:02:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="xgboost-extreme-gradient-boost">XGBoost: Extreme Gradient Boost</h1>
<h2 id="1-xgboost-contents">1. XGBoost Contents</h2>
<ol>
<li>Gradient Boost</li>
<li>Regularization</li>
<li>Unique Regression Tree/ Classification Tree</li>
<li>Approximate Greedy Algorithm</li>
<li>Weighted Quantile Sketch</li>
<li>Sparsity-Aware Split Finding</li>
<li>Paralle Learning</li>
<li>Cache-Aware Access</li>
<li>Blocks for Out-of-Core Computation</li>
</ol>
<h2 id="2-xgboost-details">2. XGBoost Details</h2>
<h4 id="21-xgboost-for-regression">2.1 XGBoost for Regression</h4>
<h6 id="211-example">2.1.1 Example</h6>
<figure data-type="image" tabindex="1"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_example.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>Make initial prediction</p>
<ul>
<li>By defalut use <code>$0.5$</code>.</li>
<li>Can use average (mean).</li>
</ul>
</li>
<li>
<p>Calculate similarity score:</p>
<pre><code class="language-math">\text{Similarity Score} = \frac{(\sum residuals)^2}{\#residuals + \lambda}
</code></pre>
<ul>
<li><code>$\lambda$</code>: <strong>regularization parameter</strong><br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_lambda.png" alt="" loading="lazy"></li>
<li>the residuals can <strong>cancel out</strong> in the score.</li>
</ul>
</li>
<li>
<p>Try all valid splits and calculate similarity scores for each child.</p>
<ul>
<li>nodes with quite different residuals -&gt; many cancel out -&gt; low similarity scores</li>
</ul>
</li>
<li>
<p>Calculate <code>$Gain = left + right - root$</code>, on similarity scores.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_similarity_gain.png" alt="" loading="lazy"></p>
</li>
<li>
<p>Compare and select the split that has the largest similarity gain.</p>
</li>
<li>
<p>Prune XGBoost tree with <code>$\gamma$</code>, a <strong>tree complexity parameter</strong>.</p>
</li>
</ol>
<pre><code class="language-math">Gain-\gamma \;
\begin{cases}
      &lt;0 &amp; \text{reomve the node}\\
      &gt;0 &amp; \text{keep the node}
\end{cases} 
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_gain_lambda.png" alt="" loading="lazy"></figure>
<ol start="7">
<li>Calculate the output value of each leaf<pre><code class="language-math">\text{Output Value} = \frac{\sum residuals}{\#residuals + \lambda}
</code></pre>
<ul>
<li>It's exaclty the same as the output value in <strong>GBDT</strong> except for the <strong>regularization parameter</strong>.</li>
<li>if <code>$\lambda&gt;0$</code> -&gt; reduce the amount of the leaf output adds to the prediction -&gt; make the prediction <strong>less sensitive</strong>.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_output.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>Keep adding trees to predict latest residuals, until reaching the maximum steps, or adding new trees doesn't significantly reduce residuals.</li>
</ol>
<h6 id="212-algorithm">2.1.2 Algorithm</h6>
<h4 id="22-xgboost-for-classification">2.2 XGBoost for Classification</h4>
<h6 id="221-example">2.2.1 Example</h6>
<figure data-type="image" tabindex="3"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_clf_example.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>Make initial prediction</p>
<ul>
<li>By defalut use <code>$0.5$</code>.</li>
<li>Can use average (mean) probability.</li>
</ul>
</li>
<li>
<p>Calculate similarity score:</p>
<pre><code class="language-math">\text{Similarity Score} = \frac{(\sum residuals)^2}{\sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)] + \lambda}
</code></pre>
<ul>
<li>The numerator is the same as that in regression.</li>
<li><code>$\lambda$</code>: <strong>regularization parameter</strong><br>
<img src="" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>Try all valid splits and calculate similarity scores for each child.</p>
</li>
<li>
<p>Calculate <code>$Gain = left + right - root$</code>, on similarity scores.</p>
</li>
<li>
<p>Compare and select the split that has the largest similarity gain.</p>
</li>
<li>
<p>Prune XGBoost tree with</p>
<ol>
<li><strong>cover</strong> = (<strong>min_child_weight</strong>?): determine minimum number of residuals in a leaf.<pre><code class="language-math">cover = \sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)]
</code></pre>
<ul>
<li>This is the <strong>Gini-Impurity</strong>.</li>
<li>For regression: <code>$cover = \# residuals$</code></li>
<li>Generally, <code>$cover = denomiator\;of\;similarity-\lambda$</code>, by default is 1.</li>
</ul>
</li>
<li>Also use <code>$gamma$</code>.</li>
</ol>
</li>
<li>
<p>Calculate the output value of each leaf</p>
<pre><code class="language-math">\text{Output Value} = \frac{(\sum residuals)}{\sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)] + \lambda}
</code></pre>
<ul>
<li>It's exaclty the same as the output value in <strong>GBDT</strong> except for the <strong>regularization parameter</strong>.</li>
<li>if <code>$\lambda&gt;0$</code> -&gt; reduce the amount of the leaf output adds to the prediction -&gt; make the prediction <strong>less sensitive</strong>.<br>
<img src="" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>Keep adding trees to predict latest residuals, until reaching the maximum steps, or adding new trees doesn't significantly reduce residuals.</p>
</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_clf_output.png" alt="" loading="lazy"></figure>
<h6 id="222-algorithm">2.2.2 Algorithm</h6>
<h2 id="3-xgboost-implementationoptimization">3. XGBoost Implementation(optimization)</h2>
<h4 id="31-approximate-greedy-algorithm">3.1 Approximate Greedy Algorithm</h4>
<ul>
<li>
<p>When searching for <strong>best splitting point</strong>, XGBoost uses <strong>greedy</strong> strategy, which doesn't take long-term outcomes. This makes the training fast.</p>
</li>
<li>
<p>It first finds the <strong>quantiles</strong> of a feature, and use them to define splitting thresholds.</p>
</li>
</ul>
<h4 id="32-parallel-learning">3.2 Parallel Learning</h4>
<p>Quantile Sketch Algorithm:</p>
<ol>
<li>Data in different machine -&gt; Approximate histogram -&gt; Approximate quantiles</li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/quantile_sketched_algorithm.png" alt="" loading="lazy"></figure>
<ol start="2">
<li>Weighted quantile: the sum of __ weights__ is the same in each quantile, which is defined as the <strong>cover</strong> &lt;-&gt; second derivative.
<ul>
<li>Regression: is all <strong>1</strong>, just like the normal quantile</li>
<li>Classification: <code>$Previous\;Probability_i(1-Previous\;Probability_i)$</code></li>
</ul>
</li>
</ol>
<h4 id="33-sparsity-aware-split-finding">3.3 Sparsity-Aware Split Finding</h4>
<ol>
<li>
<p>Find splitting point<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_missing_split.png" alt="" loading="lazy"></p>
</li>
<li>
<p>Use the strategy for all missing data in the future.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_missing_split2.png" alt="" loading="lazy"></p>
</li>
</ol>
<h4 id="34-cache-aware-access">3.4 Cache-Aware Access</h4>
<figure data-type="image" tabindex="6"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_cache_memory.png" alt="" loading="lazy"></figure>
<h2 id="4-xgboost-application">4. XGBoost Application</h2>
<h4 id="41-advantages-vs-gbm">4.1 Advantages vs. GBM</h4>
<ol>
<li>
<p>Regularization:</p>
<ul>
<li>Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.</li>
<li>In fact, XGBoost is also known as a ‘regularized boosting‘ technique.</li>
</ul>
</li>
<li>
<p>Parallel Processing:</p>
<ul>
<li>XGBoost implements parallel processing and is blazingly faster as compared to GBM.</li>
<li>But hang on, we know that boosting is a sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.</li>
<li>XGBoost also supports implementation on Hadoop.</li>
</ul>
</li>
<li>
<p>High Flexibility</p>
<ul>
<li>XGBoost allows users to define custom optimization objectives and evaluation criteria.</li>
<li>This adds a whole new dimension to the model and there is no limit to what we can do.</li>
</ul>
</li>
<li>
<p>Handling Missing Values</p>
<ul>
<li>XGBoost has an in-built routine to handle missing values.</li>
<li>The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.</li>
</ul>
</li>
<li>
<p>Tree Pruning:</p>
<ul>
<li>A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.</li>
<li>XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.</li>
<li>Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.</li>
</ul>
</li>
<li>
<p>Built-in Cross-Validation</p>
<ul>
<li>XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.</li>
<li>This is unlike GBM where we have to run a grid-search and only a limited values can be tested.</li>
</ul>
</li>
<li>
<p>Continue on Existing Model</p>
<ul>
<li>User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.</li>
<li>GBM implementation of sklearn also has this feature so they are even on this point.</li>
</ul>
</li>
</ol>
<h4 id="42-xgboost-parameters">4.2 XGBoost Parameters</h4>
<p>https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://Bill-Zhan.github.io/post/hello-gridea/</id>
        <link href="https://Bill-Zhan.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>