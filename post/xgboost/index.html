<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>XGBoost | Gridea</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://Bill-Zhan.github.io/favicon.ico?v=1611021766315">
<link rel="stylesheet" href="https://Bill-Zhan.github.io/styles/main.css">



<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="XGBoost: Extreme Gradient Boost
1. XGBoost Contents

Gradient Boost
Regularization
Unique Regression Tree/ Classificatio..." />
    <meta name="keywords" content="" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://Bill-Zhan.github.io">
        <img src="https://Bill-Zhan.github.io/images/avatar.png?v=1611021766315" class="site-logo">
        <h1 class="site-title">Gridea</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            Home
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            Archives
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            Tags
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            About
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="https://github.com/Bill-Zhan" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        
      
        
          <a class="social-link" href="https://www.linkedin.com/in/xiaotian-zhan/" target="_blank">
            <i class="fab fa-linkedin"></i>
          </a>
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      Live to Learn
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://Bill-Zhan.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">XGBoost</h2>
            <div class="post-date">2021-01-18</div>
            
            <div class="post-content" v-pre>
              <h1 id="xgboost-extreme-gradient-boost">XGBoost: Extreme Gradient Boost</h1>
<h2 id="1-xgboost-contents">1. XGBoost Contents</h2>
<ol>
<li>Gradient Boost</li>
<li>Regularization</li>
<li>Unique Regression Tree/ Classification Tree</li>
<li>Approximate Greedy Algorithm</li>
<li>Weighted Quantile Sketch</li>
<li>Sparsity-Aware Split Finding</li>
<li>Paralle Learning</li>
<li>Cache-Aware Access</li>
<li>Blocks for Out-of-Core Computation</li>
</ol>
<h2 id="2-xgboost-details">2. XGBoost Details</h2>
<h4 id="21-xgboost-for-regression">2.1 XGBoost for Regression</h4>
<h6 id="211-example">2.1.1 Example</h6>
<figure data-type="image" tabindex="1"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_example.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>Make initial prediction</p>
<ul>
<li>By defalut use <code>$0.5$</code>.</li>
<li>Can use average (mean).</li>
</ul>
</li>
<li>
<p>Calculate similarity score:</p>
<pre><code class="language-math">\text{Similarity Score} = \frac{(\sum residuals)^2}{\#residuals + \lambda}
</code></pre>
<ul>
<li><code>$\lambda$</code>: <strong>regularization parameter</strong><br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_lambda.png" alt="" loading="lazy"></li>
<li>the residuals can <strong>cancel out</strong> in the score.</li>
</ul>
</li>
<li>
<p>Try all valid splits and calculate similarity scores for each child.</p>
<ul>
<li>nodes with quite different residuals -&gt; many cancel out -&gt; low similarity scores</li>
</ul>
</li>
<li>
<p>Calculate <code>$Gain = left + right - root$</code>, on similarity scores.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_similarity_gain.png" alt="" loading="lazy"></p>
</li>
<li>
<p>Compare and select the split that has the largest similarity gain.</p>
</li>
<li>
<p>Prune XGBoost tree with <code>$\gamma$</code>, a <strong>tree complexity parameter</strong>.</p>
</li>
</ol>
<pre><code class="language-math">Gain-\gamma \;
\begin{cases}
      &lt;0 &amp; \text{reomve the node}\\
      &gt;0 &amp; \text{keep the node}
\end{cases} 
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_gain_lambda.png" alt="" loading="lazy"></figure>
<ol start="7">
<li>Calculate the output value of each leaf<pre><code class="language-math">\text{Output Value} = \frac{\sum residuals}{\#residuals + \lambda}
</code></pre>
<ul>
<li>It's exaclty the same as the output value in <strong>GBDT</strong> except for the <strong>regularization parameter</strong>.</li>
<li>if <code>$\lambda&gt;0$</code> -&gt; reduce the amount of the leaf output adds to the prediction -&gt; make the prediction <strong>less sensitive</strong>.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_reg_output.png" alt="" loading="lazy"></li>
</ul>
</li>
<li>Keep adding trees to predict latest residuals, until reaching the maximum steps, or adding new trees doesn't significantly reduce residuals.</li>
</ol>
<h6 id="212-algorithm">2.1.2 Algorithm</h6>
<h4 id="22-xgboost-for-classification">2.2 XGBoost for Classification</h4>
<h6 id="221-example">2.2.1 Example</h6>
<figure data-type="image" tabindex="3"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_clf_example.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>Make initial prediction</p>
<ul>
<li>By defalut use <code>$0.5$</code>.</li>
<li>Can use average (mean) probability.</li>
</ul>
</li>
<li>
<p>Calculate similarity score:</p>
<pre><code class="language-math">\text{Similarity Score} = \frac{(\sum residuals)^2}{\sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)] + \lambda}
</code></pre>
<ul>
<li>The numerator is the same as that in regression.</li>
<li><code>$\lambda$</code>: <strong>regularization parameter</strong><br>
<img src="" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>Try all valid splits and calculate similarity scores for each child.</p>
</li>
<li>
<p>Calculate <code>$Gain = left + right - root$</code>, on similarity scores.</p>
</li>
<li>
<p>Compare and select the split that has the largest similarity gain.</p>
</li>
<li>
<p>Prune XGBoost tree with</p>
<ol>
<li><strong>cover</strong> = (<strong>min_child_weight</strong>?): determine minimum number of residuals in a leaf.<pre><code class="language-math">cover = \sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)]
</code></pre>
<ul>
<li>This is the <strong>Gini-Impurity</strong>.</li>
<li>For regression: <code>$cover = \# residuals$</code></li>
<li>Generally, <code>$cover = denomiator\;of\;similarity-\lambda$</code>, by default is 1.</li>
</ul>
</li>
<li>Also use <code>$gamma$</code>.</li>
</ol>
</li>
<li>
<p>Calculate the output value of each leaf</p>
<pre><code class="language-math">\text{Output Value} = \frac{(\sum residuals)}{\sum_i[Previous\;Probability_i\times(1-Previous\;Probability_i)] + \lambda}
</code></pre>
<ul>
<li>It's exaclty the same as the output value in <strong>GBDT</strong> except for the <strong>regularization parameter</strong>.</li>
<li>if <code>$\lambda&gt;0$</code> -&gt; reduce the amount of the leaf output adds to the prediction -&gt; make the prediction <strong>less sensitive</strong>.<br>
<img src="" alt="" loading="lazy"></li>
</ul>
</li>
<li>
<p>Keep adding trees to predict latest residuals, until reaching the maximum steps, or adding new trees doesn't significantly reduce residuals.</p>
</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_clf_output.png" alt="" loading="lazy"></figure>
<h6 id="222-algorithm">2.2.2 Algorithm</h6>
<h2 id="3-xgboost-implementationoptimization">3. XGBoost Implementation(optimization)</h2>
<h4 id="31-approximate-greedy-algorithm">3.1 Approximate Greedy Algorithm</h4>
<ul>
<li>
<p>When searching for <strong>best splitting point</strong>, XGBoost uses <strong>greedy</strong> strategy, which doesn't take long-term outcomes. This makes the training fast.</p>
</li>
<li>
<p>It first finds the <strong>quantiles</strong> of a feature, and use them to define splitting thresholds.</p>
</li>
</ul>
<h4 id="32-parallel-learning">3.2 Parallel Learning</h4>
<p>Quantile Sketch Algorithm:</p>
<ol>
<li>Data in different machine -&gt; Approximate histogram -&gt; Approximate quantiles</li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/quantile_sketched_algorithm.png" alt="" loading="lazy"></figure>
<ol start="2">
<li>Weighted quantile: the sum of __ weights__ is the same in each quantile, which is defined as the <strong>cover</strong> &lt;-&gt; second derivative.
<ul>
<li>Regression: is all <strong>1</strong>, just like the normal quantile</li>
<li>Classification: <code>$Previous\;Probability_i(1-Previous\;Probability_i)$</code></li>
</ul>
</li>
</ol>
<h4 id="33-sparsity-aware-split-finding">3.3 Sparsity-Aware Split Finding</h4>
<ol>
<li>
<p>Find splitting point<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_missing_split.png" alt="" loading="lazy"></p>
</li>
<li>
<p>Use the strategy for all missing data in the future.<br>
<img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_missing_split2.png" alt="" loading="lazy"></p>
</li>
</ol>
<h4 id="34-cache-aware-access">3.4 Cache-Aware Access</h4>
<figure data-type="image" tabindex="6"><img src="https://github.com/Bill-Zhan/ImageRepo/raw/main/ml/xgboost/xgb_cache_memory.png" alt="" loading="lazy"></figure>
<h2 id="4-xgboost-application">4. XGBoost Application</h2>
<h4 id="41-advantages-vs-gbm">4.1 Advantages vs. GBM</h4>
<ol>
<li>
<p>Regularization:</p>
<ul>
<li>Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.</li>
<li>In fact, XGBoost is also known as a ‘regularized boosting‘ technique.</li>
</ul>
</li>
<li>
<p>Parallel Processing:</p>
<ul>
<li>XGBoost implements parallel processing and is blazingly faster as compared to GBM.</li>
<li>But hang on, we know that boosting is a sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.</li>
<li>XGBoost also supports implementation on Hadoop.</li>
</ul>
</li>
<li>
<p>High Flexibility</p>
<ul>
<li>XGBoost allows users to define custom optimization objectives and evaluation criteria.</li>
<li>This adds a whole new dimension to the model and there is no limit to what we can do.</li>
</ul>
</li>
<li>
<p>Handling Missing Values</p>
<ul>
<li>XGBoost has an in-built routine to handle missing values.</li>
<li>The user is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.</li>
</ul>
</li>
<li>
<p>Tree Pruning:</p>
<ul>
<li>A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.</li>
<li>XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.</li>
<li>Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.</li>
</ul>
</li>
<li>
<p>Built-in Cross-Validation</p>
<ul>
<li>XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.</li>
<li>This is unlike GBM where we have to run a grid-search and only a limited values can be tested.</li>
</ul>
</li>
<li>
<p>Continue on Existing Model</p>
<ul>
<li>User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.</li>
<li>GBM implementation of sklearn also has this feature so they are even on this point.</li>
</ul>
</li>
</ol>
<h4 id="42-xgboost-parameters">4.2 XGBoost Parameters</h4>
<p>https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</p>

            </div>
            
            
              <div class="next-post">
                <div class="next">Next</div>
                <a href="https://Bill-Zhan.github.io/post/hello-gridea/">
                  <h3 class="post-title">
                    Hello Gridea
                  </h3>
                </a>
              </div>
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>






  </body>
</html>
